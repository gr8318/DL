# -*- coding: utf-8 -*-
"""DL_MODEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18RM7ULW6yYd_CGEp3k-FToTNCEW0F56t
"""

#Implementing Logic Gates using Sigmoid Neuron
import numpy as np

def sigmoid(z):
 return 1 / (1 + np.exp(-z))

def and_gate(x1, x2):
 w1, w2, b = 0.5, 0.5, -0.8
 z = w1 * x1 + w2 * x2 + b
 return 1 if sigmoid(z) > 0.5 else 0

def not_gate(x):
   w, b = -1, 0.5
   z = w * x + b
   return 1 if sigmoid(z) > 0.5 else 0

def or_gate(x1, x2):
 w1, w2, b = 0.5, 0.5, -0.3
 z = w1 * x1 + w2 * x2 + b
 return 1 if sigmoid(z) > 0.5 else 0

print("--- AND Gate ---")
print(f"AND(0, 0) = {and_gate(0, 0)}")
print(f"AND(0, 1) = {and_gate(0, 1)}")
print(f"AND(1, 0) = {and_gate(1, 0)}")
print(f"AND(1, 1) = {and_gate(1, 1)}")
print("-" * 20)
print("--- OR Gate ---")
print(f"OR(0, 0) = {or_gate(0, 0)}")
print(f"OR(0, 1) = {or_gate(0, 1)}")
print(f"OR(1, 0) = {or_gate(1, 0)}")
print(f"OR(1, 1) = {or_gate(1, 1)}")
print("-" * 20)
print("--- NOT Gate ---")
print(f"NOT(0) = {not_gate(0)}")
print(f"NOT(1) = {not_gate(1)}")
print("-" * 20)

#Training a Neural Network with Backpropagation to Learn XOR Gate
import numpy as np

def sigmoid(x):
 return 1 / (1 + np.exp(-x))
def sigmoid_derivative(x):
 return x * (1 - x)

X = np.array([[0, 0],
 [0, 1],
 [1, 0],
 [1, 1]])
y = np.array([[0], [1], [1], [0]])

input_layer_neurons = 2
hidden_layer_neurons = 2
output_neurons = 1
learning_rate = 0.1
epochs = 10000

np.random.seed(42)
wh = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
bh = np.random.uniform(size=(1, hidden_layer_neurons))
wo = np.random.uniform(size=(hidden_layer_neurons, output_neurons))
bo = np.random.uniform(size=(1, output_neurons))

for epoch in range(epochs):

 hidden_input = np.dot(X, wh) + bh
 hidden_output = sigmoid(hidden_input)
 final_input = np.dot(hidden_output, wo) + bo
 predicted_output = sigmoid(final_input)

 error = y - predicted_output
 d_predicted_output = error * sigmoid_derivative(predicted_output)
 error_hidden_layer = d_predicted_output.dot(wo.T)
 d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_output)

 wo += hidden_output.T.dot(d_predicted_output) * learning_rate
 bo += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
 wh += X.T.dot(d_hidden_layer) * learning_rate
 bh += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

print("Final output after training:")
print(np.round(predicted_output, 3))

#Simple Perceptron Model for AND Logic Gate
import numpy as np
class Perceptron:
 def __init__(self, learning_rate=0.01, n_iters=10):
  self.lr = learning_rate
  self.n_iters = n_iters
  self.weights = None
  self.bias = None
 def fit(self, x, y):
  n_samples, n_features = x.shape
  self.weights = np.zeros(n_features)
  self.bias = 0
  for _ in range(self.n_iters):
   for idx, x_i in enumerate(x):
    linear_output = np.dot(x_i, self.weights) + self.bias
    y_predicted = self.activation(linear_output)
    update = self.lr * (y[idx] - y_predicted)
    self.weights += update * x_i
    self.bias += update
 def activation(self, x):
  return 1 if x >= 0 else 0
 def predict(self, x):
  linear_output = np.dot(x, self.weights) + self.bias
  y_predicted = np.array([self.activation(val) for val in linear_output])
  return y_predicted

if __name__ == "__main__":

 x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
 y = np.array([0, 0, 0, 1])
 p = Perceptron(learning_rate=0.1, n_iters=10)
 p.fit(x, y)
 predictions = p.predict(x)
 print("Predictions:", predictions)

#Classification of Iris Dataset using a Single Layer Perceptron (SLP) with TensorFlow/Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np

iris = load_iris()
X, y = iris.data, iris.target

scaler = StandardScaler()
X = scaler.fit_transform(X)

encoder = LabelEncoder()
y = encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=0.3, random_state=42
)

model = keras.Sequential([
 layers.Dense(10, activation='relu', input_shape=(4,)),
 layers.Dense(3, activation='softmax')
])

model.compile(
 optimizer='adam',
 loss='sparse_categorical_crossentropy',
 metrics=['accuracy']
)

history = model.fit(
 X_train, y_train,
  epochs=50,
 validation_data=(X_test, y_test),
 verbose=1
)

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest accuracy: {test_acc:.4f}")

#Simple Perceptron Model for Binary Classification on Iris Dataset
import numpy as np
from sklearn.datasets import load_iris
class Perceptron:
 def __init__(self, learning_rate=0.01, n_iters=10):
  self.lr = learning_rate
  self.n_iters = n_iters
  self.weights = None
  self.bias = None
 def fit(self, X, y):
  n_samples, n_features = X.shape
  self.weights = np.zeros(n_features)
  self.bias = 0
  for _ in range(self.n_iters):
   for idx, x_i in enumerate(X):
    linear_output = np.dot(x_i, self.weights) + self.bias
    y_predicted = self.activation(linear_output)
    update = self.lr * (y[idx] - y_predicted)
    self.weights += update * x_i
    self.bias += update
 def activation(self, x):
  return 1 if x >= 0 else 0
 def predict(self, X):
  if X.ndim == 1:
   linear_output = [np.dot(X, self.weights) + self.bias]
  else:
   linear_output = np.dot(X, self.weights) + self.bias
  y_predicted = np.array([self.activation(val) for val in linear_output])
  return y_predicted
if __name__ == "__main__":
 iris = load_iris()
 X = iris.data
 y = iris.target
 y = np.where(y == 0, 0, 1)
 p = Perceptron(learning_rate=0.1, n_iters=10)
 p.fit(X, y)
 predictions = p.predict(X)
 print("Predictions:", predictions)

#Handwritten Digit Recognition on MNIST using a Multilayer Perceptron
import tensorflow as tf
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
20

X_train = X_train / 255.0
X_test = X_test / 255.0

X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)

mlp = MLPClassifier(
 hidden_layer_sizes=(100,),
 max_iter=10,
 solver='adam',
 random_state=42
)

mlp.fit(X_train, y_train)

y_pred = mlp.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
21
print("Classification Report:")
print(classification_report(y_test, y_pred))

fig, axes = plt.subplots(2, 5, figsize=(10, 5))
axes = axes.ravel()
for i in range(10):
 axes[i].imshow(X_test[i].reshape(28, 28), cmap='gray')
 axes[i].set_title(f'Pred: {y_pred[i]}')
 axes[i].axis('off')
plt.tight_layout()
plt.show()

#Predicting Temperature Over Time Using RBF Regression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

time = np.array([6, 9, 12, 15, 18, 21]).reshape(-1, 1)
temperature = np.array([15, 20, 30, 32, 25, 18])

def rbf(x, c, gamma=0.1):
 return np.exp(-gamma * (x - c)**2)

centers = time.flatten()
gamma = 0.1

X_rbf = np.zeros((len(time), len(centers)))
for i, c in enumerate(centers):
 X_rbf[:, i] = rbf(time.flatten(), c, gamma)

model = LinearRegression()
model.fit(X_rbf, temperature)

time_test = np.linspace(0, 24, 100).reshape(-1, 1)
X_rbf_test = np.zeros((len(time_test), len(centers)))
for i, c in enumerate(centers):
 X_rbf_test[:, i] = rbf(time_test.flatten(), c, gamma)

temp_pred = model.predict(X_rbf_test)

plt.figure(figsize=(8, 5))
plt.scatter(time, temperature, color='red', label='Training Data')
plt.plot(time_test, temp_pred, label='RBF Regression Prediction', color='blue')
plt.title('Predicting Temperature Over Time Using RBF')
plt.xlabel('Time of Day (Hours)')
plt.ylabel('Temperature (Â°C)')
plt.legend()
plt.grid(True)
plt.show()

#Solving XOR Problem Using a Neural Network (TensorFlow/Keras)
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

X = np.array([[0, 0],
 [0, 1],
 [1, 0],
 [1, 1]], dtype=np.float32)
y = np.array([0, 1, 1, 0], dtype=np.float32)

model = Sequential([
 Dense(4, activation='relu', input_shape=(2,)),
 Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(learning_rate=0.1),
 loss='binary_crossentropy',
 metrics=['accuracy'])

print("Training progress:")
history = model.fit(X, y, epochs=1000, verbose=0)

for i in range(0, 1000, 100):
 loss, acc = history.history['loss'][i], history.history['accuracy'][i]
 print(f"Epoch {i}: loss = {loss:.4f}, accuracy = {acc:.4f}")

predictions = model.predict(X)
predicted_classes = (predictions > 0.5).astype(int)
print("\nFinal Predictions:")
for i in range(len(X)):
 print((f"Input: {X[i]}, Predicted: {predicted_classes[i][0]} "
  f"(prob: {predictions[i][0]:.4f}), Actual: {y[i]}"))

print("\nModel architecture:")
model.summary()

#Implementing XOR Problem Using a Custom Neural Network (NumPy)
import numpy as np

class NeuralNetwork:
 def __init__(self, input_size, hidden_size, output_size):
  np.random.seed(42)

  self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)
  self.b1 = np.zeros((1, hidden_size))
  self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)
  self.b2 = np.zeros((1, output_size))

 def sigmoid(self, z):
  return 1 / (1 + np.exp(-z))

 def forward(self, X):
  self.z1 = np.dot(X, self.W1) + self.b1
  self.a1 = np.maximum(0, self.z1)
  self.z2 = np.dot(self.a1, self.W2) + self.b2
  self.a2 = self.sigmoid(self.z2)
  return self.a2

 def backward(self, X, y, lr=0.1):
  m = X.shape[0]
  dz2 = self.a2 - y.reshape(-1, 1)
  dW2 = np.dot(self.a1.T, dz2) / m
  db2 = np.sum(dz2, axis=0, keepdims=True) / m
  dz1 = np.dot(dz2, self.W2.T) * (self.z1 > 0)
  dW1 = np.dot(X.T, dz1) / m
  db1 = np.sum(dz1, axis=0, keepdims=True) / m

  self.W1 -= lr * dW1
  self.b1 -= lr * db1
  self.W2 -= lr * dW2
  self.b2 -= lr * db2

 def train(self, X, y, epochs=1000, lr=0.1):
  for epoch in range(epochs):
   self.forward(X)
   self.backward(X, y, lr)
   if epoch % 100 == 0:
    loss = -np.mean(y * np.log(self.a2 + 1e-8) + (1 - y) * np.log(1 - self.a2 + 1e-8))
    print(f"Epoch {epoch}, Loss: {loss:.4f}")

 def predict(self, X, threshold=0.5):
  proba = self.forward(X)
  return (proba >= threshold).astype(int)

X = np.array([[0, 0],
 [0, 1],
 [1, 0],
 [1, 1]])
y = np.array([0, 1, 1, 0])

nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
nn.train(X, y, epochs=1000, lr=0.1)

predictions = nn.predict(X)
print("\nFinal Predictions:")
for i in range(len(X)):
 print(f"Input: {X[i]}, Predicted: {predictions[i][0]}, Actual: {y[i]}")

#CIFAR-10 Classification with a Simple Fully Connected Neural Network (TensorFlow/Keras)
import numpy as np
import tensorflow as tf
from tensorflow.keras import datasets, models, layers
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

train_images = train_images.reshape((train_images.shape[0], -1)) / 255.0
test_images = test_images.reshape((test_images.shape[0], -1)) / 255.0

class_names = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer',
 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']

model = models.Sequential([
 layers.Dense(512, activation='relu', input_shape=(3072,)),
 layers.Dense(256, activation='relu'),
 layers.Dense(10)
])

model.compile(
 optimizer='adam',
 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
 metrics=['accuracy']
)

model.fit(
 train_images, train_labels,
 epochs=10,
 batch_size=64,
 validation_data=(test_images, test_labels),
 verbose=2
)

test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f"Test accuracy: {test_acc:.2f}")

probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])
predictions = probability_model.predict(test_images, verbose=0)
print(f"Predicted label: {class_names[np.argmax(predictions[0])]} | "
 f"True label: {class_names[test_labels[0][0]]}")

num_images_to_show = 5
plt.figure(figsize=(10, 5))
for i in range(num_images_to_show):
 plt.subplot(1, num_images_to_show, i + 1)
 plt.xticks([])
 plt.yticks([])
 plt.grid(False)

 plt.imshow(test_images[i].reshape(32, 32, 3))
 pred_label = class_names[np.argmax(predictions[i])]
 true_label = class_names[test_labels[i][0]]

 color = 'green' if pred_label == true_label else 'red'
 plt.title(pred_label, color=color)
 plt.xlabel(f"True: {true_label}")
plt.tight_layout()
plt.show()

#Handwritten Digit Classification on MNIST using CNN (TensorFlow/Keras)
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import numpy as np
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()

x_train = x_train.reshape(-1, 28, 28, 1).astype("float32") / 255.0
x_test = x_test.reshape(-1, 28, 28, 1).astype("float32") / 255.0
40

model = models.Sequential([
 layers.Conv2D(32, (3, 3), activation="relu", input_shape=(28, 28, 1)),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(64, (3, 3), activation="relu"),
 layers.MaxPooling2D((2, 2)),
 layers.Flatten(),
 layers.Dense(128, activation="relu"),
 layers.Dense(10, activation="softmax")
])

model.compile(
 optimizer="adam",
 loss="sparse_categorical_crossentropy",
 metrics=["accuracy"]
)

model.fit(x_train, y_train, epochs=5, validation_split=0.1)

loss, acc = model.evaluate(x_test, y_test, verbose=0)
print(f"\nTest Accuracy: {acc*100:.2f}%")

img = x_test[4]
plt.imshow(img.squeeze(), cmap="gray")
plt.title("Input Digit")
plt.show()
pred = model.predict(np.expand_dims(img, axis=0), verbose=0)
print("Predicted digit:", pred.argmax())

#Image Classification using Pre-Trained VGG16 (Transfer Learning on ImageNet)
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import numpy as np

model = VGG16(weights='imagenet')

img_path = "/content/OIP (1).jfif"
try:
 img = image.load_img(img_path, target_size=(224, 224))
except FileNotFoundError:
 print(f"Error: Image file not found at {img_path}")
 exit()
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

preds = model.predict(x, verbose=0)

plt.imshow(img)
plt.title("Input Image")
plt.axis('off')
plt.show()

print('Predicted:', decode_predictions(preds, top=3)[0])